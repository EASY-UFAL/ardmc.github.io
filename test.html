<html>
    <head>
        <link href="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.css" rel="stylesheet">
        <script src="https://unpkg.com/material-components-web@latest/dist/material-components-web.min.js"></script>
        <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js" crossorigin="anonymous"></script>
        <style>
            body {
            font-family: roboto;
            margin: 2em;
            color: #3d3d3d;
            --mdc-theme-primary: #007f8b;
            --mdc-theme-on-primary: #f1f3f4;
            }

            h1 {
            color: #007f8b;
            }

            h2 {
            clear: both;
            }

            video {
            clear: both;
            display: block;
            transform: rotateY(180deg);
            -webkit-transform: rotateY(180deg);
            -moz-transform: rotateY(180deg);
            height: 280px;
            }

            section {
            opacity: 1;
            transition: opacity 500ms ease-in-out;
            }

            .removed {
            display: none;
            }

            .invisible {
            opacity: 0.2;
            }

            .detectOnClick {
            position: relative;
            float: left;
            width: 48%;
            margin: 2% 1%;
            cursor: pointer;
            }
            .videoView {
            position: absolute;
            float: left;
            width: 48%;
            margin: 2% 1%;
            cursor: pointer;
            min-height: 500px;
            }

            .videoView p,
            .detectOnClick p {
            padding-top: 5px;
            padding-bottom: 5px;
            background-color: #007f8b;
            color: #fff;
            border: 1px dashed rgba(255, 255, 255, 0.7);
            z-index: 2;
            margin: 0;
            }

            .highlighter {
            background: rgba(0, 255, 0, 0.25);
            border: 1px dashed #fff;
            z-index: 1;
            position: absolute;
            }

            .canvas {
            z-index: 1;
            position: absolute;
            pointer-events: none;
            }

            .output_canvas {
            transform: rotateY(180deg);
            -webkit-transform: rotateY(180deg);
            -moz-transform: rotateY(180deg);
            }

            .detectOnClick {
            z-index: 0;
            font-size: calc(8px + 1.2vw);
            }

            .detectOnClick img {
            width: 45vw;
            }
            .output {
            display: none;
            width: 100%;
            font-size: calc(8px + 1.2vw);
            }
        </style>
    </head>
    <body>
        <div id="liveView" class="videoView">
            <div style="position: relative;">
                <video id="webcam" autoplay playsinline></video>
                <canvas class="output_canvas" id="output_canvas" width="2560" height="1440"
                    style="position: absolute; left: 0px; top: 0px;"></canvas>
                <p id='gesture_output' class="output">
            </div>
        </div>
        </section>
    </body>

    <script type="module">
        import {
                GestureRecognizer,
                FilesetResolver,
                DrawingUtils
            } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0";

        const demosSection = document.getElementById("demos");
        let gestureRecognizer;
        let enableWebcamButton;
        const videoHeight = "520px";
        const videoWidth = "760px";

        // Before we can use HandLandmarker class we must wait for it to finish
        // loading. Machine Learning models can be large and take a moment to
        // get everything needed to run.
        const createGestureRecognizer = async () => {
            const vision = await FilesetResolver.forVisionTasks(
                "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.0/wasm"
            );
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath:
                        "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task"
                },
                selfieMode: true,
                runningMode: "VIDEO"
            });
            // demosSection.classList.remove("invisible");
        };
        createGestureRecognizer();

        const video = document.getElementById("webcam");
        const canvasElement = document.getElementById("output_canvas");
        const canvasCtx = canvasElement.getContext("2d");
        const gestureOutput = document.getElementById("gesture_output");

        video.classList.toggle('selfie', false);

        // getUsermedia parameters.
        const constraints = {
            video: true
        };

        // Activate the webcam stream.
        navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
            video.srcObject = stream;
            video.addEventListener("loadeddata", predictWebcam);
        });

        let lastVideoTime = -1;
        let results = undefined;
        async function predictWebcam() {
            const webcamElement = document.getElementById("webcam");
            // Now let's start detecting the stream.
            let nowInMs = Date.now();
            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                results = gestureRecognizer.recognizeForVideo(video, nowInMs);
            }

            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

            canvasElement.style.height = videoHeight;
            webcamElement.style.height = videoHeight;
            canvasElement.style.width = videoWidth;
            webcamElement.style.width = videoWidth;
            if (results.landmarks) {
                for (const landmarks of results.landmarks) {
                    drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, {
                        color: "#00FF00",
                        lineWidth: 5
                    });
                    drawLandmarks(canvasCtx, landmarks, { color: "#FF0000", lineWidth: 2 });
                }
            }
            canvasCtx.restore();
            if (results.gestures.length > 0) {
                gestureOutput.style.display = "block";
                gestureOutput.style.width = videoWidth;
                const categoryName = results.gestures[0][0].categoryName;
                const categoryScore = parseFloat(
                    results.gestures[0][0].score * 100
                ).toFixed(2);
                gestureOutput.innerText = `GestureRecognizer: ${categoryName}\n Confidence: ${categoryScore} %`;
            } else {
                gestureOutput.style.display = "none";
            }
            // Call this function again to keep predicting when the browser is ready.
            window.requestAnimationFrame(predictWebcam);
        }
    </script>
</html>